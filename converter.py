import whisper
import torch
import time
import os

class WhisperConverter:
    """Whisper ìŒì„± ë³€í™˜ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, progress_callback=None, cancel_callback=None):
        self.progress_callback = progress_callback
        self.cancel_callback = cancel_callback
        self.is_cancelled = False
        self.start_time = None
        self.model = None  # ëª¨ë¸ ìºì‹±ì„ ìœ„í•œ ë³€ìˆ˜
        
    def convert_audio(self, audio_path, output_path, model_size, optimize_speed=True):
        """ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            self.start_time = time.time()
            self.is_cancelled = False
            
            # ì§„í–‰ë¥  ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
            self.last_percentage = 0
            self.last_elapsed_time = 0
            
            # 1ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ (10-30%)
            self._update_progress("ğŸ”§ Whisper ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...", 10)
            if self._is_cancelled():
                return None
            
            # ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ ë¡œë“œ
            if self.model is None or not hasattr(self.model, 'name') or self.model.name != model_size:
                self.model = whisper.load_model(model_size)
                self._update_progress("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", 20)
            else:
                self._update_progress("âœ… ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©", 20)
            
            # 2ë‹¨ê³„: GPU ì„¤ì • ë° ìµœì í™” (30-40%)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self._update_progress(f"ğŸš€ {device.upper()}ì—ì„œ ëª¨ë¸ì„ ì‹¤í–‰ ì¤‘...", 30)
            
            self.model = self.model.to(device)
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            if device == "cuda":
                torch.cuda.empty_cache()
                self._update_progress("ğŸ’¾ GPU ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ", 35)
            else:
                self._update_progress("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰", 35)
            
            if self._is_cancelled():
                return None
            
            # 3ë‹¨ê³„: ë³€í™˜ ì˜µì…˜ ì„¤ì • (40-50%)
            self._update_progress("âš™ï¸ ë³€í™˜ ì˜µì…˜ì„ ì„¤ì •í•˜ëŠ” ì¤‘...", 40)
            
            transcribe_options = {
                "language": "ko",
                "verbose": False,
                "fp16": device == "cuda",  # GPUì—ì„œ 16ë¹„íŠ¸ ì •ë°€ë„ ì‚¬ìš©
                "temperature": 0.0,  # ê²°ì •ì  ì¶œë ¥ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                "compression_ratio_threshold": 2.4,  # ì••ì¶• ë¹„ìœ¨ ì„ê³„ê°’
                "logprob_threshold": -1.0,  # ë¡œê·¸ í™•ë¥  ì„ê³„ê°’
                "no_speech_threshold": 0.6,  # ë¬´ìŒ ì„ê³„ê°’
            }
            
            # ì†ë„ ìµœì í™” ì˜µì…˜ ì¶”ê°€
            if optimize_speed:
                transcribe_options.update({
                    "beam_size": 1,  # ë¹” ê²€ìƒ‰ í¬ê¸° ìµœì†Œí™”
                    "best_of": 1,    # ìµœì  í›„ë³´ ìˆ˜ ìµœì†Œí™”
                    "patience": 1,   # ë¹” ê²€ìƒ‰ ì¸ë‚´ì‹¬ ìµœì†Œí™”
                })
                self._update_progress("âš¡ ì†ë„ ìµœì í™” ì˜µì…˜ ì ìš©", 45)
            else:
                self._update_progress("ğŸ¯ ì •í™•ë„ ìš°ì„  ì˜µì…˜ ì ìš©", 45)
            
            # 4ë‹¨ê³„: ìŒì„± íŒŒì¼ ë¶„ì„ (50-80%)
            self._update_progress("ğŸµ ìŒì„± íŒŒì¼ì„ ë¶„ì„í•˜ëŠ” ì¤‘...", 50)
            if self._is_cancelled():
                return None
            
            # ìŒì„± íŒŒì¼ ë³€í™˜ (ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¶€ë¶„)
            self._update_progress("ğŸ”„ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘... (ì´ ë‹¨ê³„ê°€ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)", 60)
            result = self.model.transcribe(audio_path, **transcribe_options)
            
            if self._is_cancelled():
                return None
            
            # 5ë‹¨ê³„: ê²°ê³¼ í›„ì²˜ë¦¬ (80-95%)
            self._update_progress("ğŸ“ ë³€í™˜ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ëŠ” ì¤‘...", 80)
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë“±)
            cleaned_text = result["text"].strip()
            
            # 6ë‹¨ê³„: íŒŒì¼ ì €ì¥ (95-100%)
            self._update_progress("ğŸ’¾ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ëŠ” ì¤‘...", 90)
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(cleaned_text)
            
            self._update_progress("ğŸ‰ ë³€í™˜ ì™„ë£Œ!", 100)
            return cleaned_text
            
        except Exception as e:
            self._update_progress(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", 0)
            raise e
    
    def _update_progress(self, message, percentage):
        """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸"""
        if self.progress_callback:
            elapsed_time = time.time() - self.start_time if self.start_time else 0
            
            # ë” ì •í™•í•œ ì‹œê°„ ì˜ˆì¸¡ (ì´ì „ ì§„í–‰ë¥ ê³¼ ë¹„êµ)
            if hasattr(self, 'last_percentage') and hasattr(self, 'last_elapsed_time'):
                if percentage > self.last_percentage:
                    # ì§„í–‰ë¥ ì´ ì¦ê°€í•œ ê²½ìš°ì—ë§Œ ì‹œê°„ ì˜ˆì¸¡ ì—…ë°ì´íŠ¸
                    progress_increase = percentage - self.last_percentage
                    time_increase = elapsed_time - self.last_elapsed_time
                    
                    if progress_increase > 0 and time_increase > 0:
                        # í˜„ì¬ ì†ë„ë¡œ ë‚¨ì€ ì§„í–‰ë¥ ì„ ì™„ë£Œí•˜ëŠ”ë° í•„ìš”í•œ ì‹œê°„
                        remaining_progress = 100 - percentage
                        estimated_remaining = (remaining_progress / progress_increase) * time_increase
                    else:
                        estimated_remaining = 0
                else:
                    estimated_remaining = 0
            else:
                # ì²« ë²ˆì§¸ ì—…ë°ì´íŠ¸ì¸ ê²½ìš°
                estimated_remaining = (elapsed_time / percentage * 100) - elapsed_time if percentage > 0 else 0
            
            # ì´ì „ ê°’ ì €ì¥
            self.last_percentage = percentage
            self.last_elapsed_time = elapsed_time
            
            self.progress_callback(message, percentage, elapsed_time, estimated_remaining)
    
    def _is_cancelled(self):
        """ì·¨ì†Œ ì—¬ë¶€ í™•ì¸"""
        if self.cancel_callback:
            self.is_cancelled = self.cancel_callback()
        return self.is_cancelled
    
    def cancel(self):
        """ë³€í™˜ ì·¨ì†Œ"""
        self.is_cancelled = True
    
    def clear_model_cache(self):
        """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        if self.model is not None:
            del self.model
            self.model = None
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    @staticmethod
    def get_optimization_tips():
        """ì†ë„ ìµœì í™” íŒ ë°˜í™˜"""
        tips = [
            "ğŸ¯ **ëª¨ë¸ í¬ê¸°**: base ëª¨ë¸ì´ ê°€ì¥ ë¹ ë¦…ë‹ˆë‹¤",
            "âš¡ **GPU ì‚¬ìš©**: CUDA GPUê°€ ìˆìœ¼ë©´ CPUë³´ë‹¤ 3-5ë°° ë¹ ë¦…ë‹ˆë‹¤",
            "ğŸ“ **íŒŒì¼ í¬ê¸°**: ì‘ì€ íŒŒì¼ì¼ìˆ˜ë¡ ë¹ ë¦…ë‹ˆë‹¤",
            "ğŸ”§ **ìµœì í™” ì˜µì…˜**: ì†ë„ ìµœì í™” ì˜µì…˜ì„ í™œì„±í™”í•˜ì„¸ìš”",
            "ğŸ’¾ **ë©”ëª¨ë¦¬**: ì¶©ë¶„í•œ RAMê³¼ VRAMì„ í™•ë³´í•˜ì„¸ìš”",
            "ğŸ”„ **ìºì‹±**: ê°™ì€ ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ë©´ ë¡œë”© ì‹œê°„ì´ ë‹¨ì¶•ë©ë‹ˆë‹¤"
        ]
        return tips 